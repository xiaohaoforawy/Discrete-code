* wait Ameba are coming!
* 三种常见的替代损失函数：hinge损失，指数损失，对率损失
* 软间隔不要求所有样本都被正确划分
* 讨论线性判别分析与线性核支持向量机在何种情况下等价。

* 首先，如果可以使用软间隔的线性SVM，其实线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。这个题只说了是线性SVM，就没必要关心数据是不是可分，毕竟LDA是都可以处理的。 第二，假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。当SVM的支持向量固定时，再加入新的样本，并不会改变求出的w，但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的w就不垂直了，但是数据依然是可分的。所以我上面说的垂直是有问题的。 我认为这个题的答案应该就是，当线性SVM和LDA求出的w互相垂直时，两者是等价的，SVM这个时候也就比LDA多了个偏移b而已。
* 试述高斯核SVM与RBF神经网络的联系

* RBF网络的径向基函数与SVM都可以采用高斯核，也就分别得到了高斯核RBF网络与高斯核SVM。 神经网络是最小化累计误差，将参数作为惩罚项，而SVM相反，主要是最小化参数，将误差作为惩罚项。 在二分类问题中，如果将RBF中隐层数为样本个数，且每个样本中心就是样本参数，得出的RBF网络与核SVM基本等价，非支持向量将得到很小的w. 使用LIBSVM对异或问题训练一个高斯核SVM得到α，修改第5章RBF网络的代码，固定β参数为高斯核SVM的参数，修改每个隐层神经元的中心为各个输入参数，得到结果w,w与α各项成正比例。
* 试析SVM对噪声敏感的原因。

* SVM的目的是求出与支持向量有最大化距离的直线，以每个样本为圆心，该距离为半径做圆，可以近似认为圆内的点与该样本属于相同分类。如果出现了噪声，那么这个噪声所带来的错误分类也将最大化，所以SVM对噪声是很敏感的。



